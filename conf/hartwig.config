/*
nf-core/oncoanalyser: A comprehensive genomic and transcriptomic analysis pipeline using the WiGiTS toolkit of the Hartwig Medical Foundation.

This example config file is not intended to be a working config, but to list some options for most use cases of oncoanalyser.

It may be useful to explore configs others have created (see: https://github.com/nf-core/configs/tree/master/conf) in case the options
listed in this config file do not solve your specific troubleshooting needs.
*/

// -----------------------------------------------------------------------------------------------------------------------------------------
// General Nexflow parameters
// -----------------------------------------------------------------------------------------------------------------------------------------

// Working directory by default is the directory where the pipeline is launched, but this can be overriden like so:
workDir = '/path/to/work/'

// Enable the use of previously cached task executions:
resume = true

// All parameters detailed are here: https://www.nextflow.io/docs/latest/reference/config.html#config-options

// -----------------------------------------------------------------------------------------------------------------------------------------
// Basic oncoanalyser parameters
// -----------------------------------------------------------------------------------------------------------------------------------------

params {
    input = '/path/to/samplesheet/'
    outdir = '/path/to/outdir'

    mode = 'wgts'                     // Can be 'wgts' or 'targeted'
    genome = 'GRCh38_hmf'             // Can be GRCh37_hmf or GRCh37_hmf
    monochromeLogs = false            // To see colored logs, set to false
    // prepare_reference_only = false // If set to true, will only download and stage the resources for the run triggered and exit
}

// All parameters are detailed here: https://nf-co.re/oncoanalyser/parameters

// -----------------------------------------------------------------------------------------------------------------------------------------
// Reference data paths
// -----------------------------------------------------------------------------------------------------------------------------------------

// Genome and WiGiTS tool reference data

params {
    genomes {
        'GRCh38_hmf' {
            fasta         = "/path/to/Homo_sapiens_assembly38.alt.masked.fasta"
            fai           = "/path/to/Homo_sapiens_assembly38.alt.masked.fasta.fai"
            dict          = "/path/to/Homo_sapiens_assembly38.alt.masked.fasta.dict"
            img           = "/path/to/Homo_sapiens_assembly38.alt.masked.fasta.img"

            bwamem2_index = "/path/to/bwa-mem2_index/" // Required if aligning DNA reads from FASTQ files
            gridss_index  = "/path/to/gridss_index/"   // Required if running VIRUSBreakend
            star_index    = "/path/to/star_index/"     // Required if aligning RNA reads from FASTQ files
        }
    }

    ref_data_hmf_data_path = "/path/to/hmf_pipeline_resources/" // Always required - will be staged from remote if not provided
}

// Panel reference data paths. Only required when running oncoanalyser with `--mode targeted`

params {
    panel = "custom_panel"

    // Root panel resources path
    ref_data_panel_data_path = "/path/to/panel_resources/"

    // Paths relative to within the dir provided by `ref_data_panel_data_path`
    panel_data_paths {

        custom_panel { // Panel name should match the one provided to the `panel`

            '38' { // Can be '37' or '38'

                driver_gene_panel           = 'common/DriverGenePanel.custom_panel.38.tsv'
                sage_actionable_panel       = 'variants/ActionableCodingPanel.custom_panel.38.bed.gz'
                sage_coverage_panel         = 'variants/CoverageCodingPanel.custom_panel.38.bed.gz'
                pon_artefacts               = 'variants/pon_artefacts.custom_panel.38.tsv.gz'
                target_region_bed           = 'copy_number/target_regions_definition.custom_panel.38.bed.gz'
                target_region_normalisation = 'copy_number/cobalt_normalisation.custom_panel.38.tsv'
                target_region_ratios        = 'copy_number/target_regions_ratios.custom_panel.38.tsv'
                target_region_msi_indels    = 'copy_number/target_regions_msi_indels.custom_panel.38.tsv'

                // Optional. If no RNA in panel, these can be omitted by providing in empty list, e.g.:
                // isofox_tpm_norm = []
                isofox_tpm_norm             = 'rna_resources/isofox.gene_normalisation.custom_panel.38.csv'
                isofox_gene_ids             = 'rna_resources/custom_panel.rna_gene_ids.csv'
                isofox_counts               = 'rna_resources/read_93_exp_counts.38.csv'
                isofox_gc_ratios            = 'rna_resources/read_93_exp_gc_ratios.38.csv'
            }
        }
    }
}

// -----------------------------------------------------------------------------------------------------------------------------------------
// Unique molecular identifier (UMI) processing options
// -----------------------------------------------------------------------------------------------------------------------------------------

params {

    // Default values are shown

    fastp_umi = false               // Enable UMI processing by fastp. Set to true if FASTQ reads have UMIs
    fastp_umi_location = "per_read" // --umi_loc fastp arg.
    fastp_umi_length = 7            // --umi_len fastp arg
    fastp_umi_skip = 0              // --umi_skip fastp arg

    redux_umi = false               // Enable UMI processing by REDUX. Set to true if BAM reads have UMIs
    redux_umi_duplex_delim = "_"    // Duplex UMI delimiter
}

// -----------------------------------------------------------------------------------------------------------------------------------------
// Compute resources
// -----------------------------------------------------------------------------------------------------------------------------------------

// Set upper limit on resources that oncoanalyser is allowed to use
params {
    max_cpus = 64
    max_memory = 256.GB
    max_time = 72.h
}

// or alternatively:
process {
    resourceLimits = [
        cpus: 64,
        memory: 256.GB,
        time: 72.h
    ]
}

// By default each process requests compute resources based on a pre-defined label, e.g. 'process_high'.
// All labels can be viewed here: https://github.com/nf-core/oncoanalyser/blob/master/conf/base.config
// Cmpute resources requested by the label can be overriden like so:

withLabel:process_high {
    cpus = 32
    memory = 128.GB
    time = 24.h
    disk = 1024.GB
}

// Alternatively, we can override based on the exact process name:
process {
   withName: 'SAGE_SOMATIC' {
      memory = 128.GB
   }
}

// or a regular expression:
process {
   withName: 'SAGE.*' {
      memory = 128.GB
   }
}

// Process names can be found at the top of the main.nf files in the modules/ dir, e.g.
// https://github.com/nf-core/oncoanalyser/blob/master/modules/local/sage/somatic/main.nf

// -----------------------------------------------------------------------------------------------------------------------------------------
// Container engines
// -----------------------------------------------------------------------------------------------------------------------------------------

// Container engines can be selected like this...
params {
    containerEngine = 'docker'
}

params {
    containerEngine = 'singularity'
}

// ...or like this
singularity {
    enabled = true
}

docker {
    enabled = true
}

// Below are some useful container platform agnostic options
singularity {

    // Args passed to `singularity exec`. Add dir
    runOptions = "-B </path/to/desired/mounted/volume/>"

    // Pulling containers may fail if it takes too long. We can increase the allowed pull time like so:
    pullTimeout = '2h'
}

// Increasing the HTTP download timeout also helps when pulling containers (or staging reference data) takes too long:
env {
    NXF_HTTP_TIMEOUT = '2h'
}

// Some options are container platform specific
singularity {
    // Directory where singularity will pull images to
    cacheDir = '/path/to/cache_dir/'

    // Automatically mounts host paths in the executed container
    autoMounts = true
}

// All container options can be viewed here: https://www.nextflow.io/docs/latest/reference/config.html

// -----------------------------------------------------------------------------------------------------------------------------------------
// Executors
// -----------------------------------------------------------------------------------------------------------------------------------------

// HPC environments often use their own job scheduler, e.g. SLURM which you can enable like so:
process {
    executor = "slurm"
}

// Below are some potentially useful SLURM options.
executor {
    queueSize         = 100
    queueStatInterval = '10 sec'
    pollInterval      = '10 sec'
    submitRateLimit   = '10 sec'
}

// Other executors include AWS batch, Google cloud batch, among others. All available executors are listed here:
// https://www.nextflow.io/docs/latest/executor.html#executors

// Each executor has their own options which can be view here:
// https://www.nextflow.io/docs/latest/reference/config.html#executor

// -----------------------------------------------------------------------------------------------------------------------------------------
// Error handling
// -----------------------------------------------------------------------------------------------------------------------------------------

// We can use errorStrategy and maxRetries to determine how Oncoanalyser proceeds when encountering an error.
// For example, to retry 3 times on any error for any process:

process {
    errorStrategy = 'retry' // Valid values can be viewed here: https://www.nextflow.io/docs/latest/reference/process.html#errorstrategy
    maxRetries = 3
}

// You may want to increase the compute resources upon each retry:

process {

    cpus   = { check_max( 1    * task.attempt, 'cpus'   ) }
    memory = { check_max( 6.GB * task.attempt, 'memory' ) }
    time   = { check_max( 4.h  * task.attempt, 'time'   ) }

    // Currently, all of the hmftools return a exit code of 1 on failure. All other exit codes relate to Nextflow or the environment.
    errorStrategy = { task.exitStatus != 1 ? 'retry' : 'finish' }
}
