/*
This shared configuration was written by the Hartwig Medical Foundation to cover most use cases for the nf-core's oncoanalyser pipeline 
developed by Stephen C. Watts. The pipeline is designed to be a comprehensive cancer DNA/RNA analysis and reporting pipeline using the Hartwig Medical Foundation's toolsuite.

This pipeline can be ran on any environment and below are provided some potentially relevant argument for each environment.
*/

// This is how container engines can be selected

docker{
  enabled = true
}

//OR 

params {
  containerEngine = 'docker'
}

// Both have the same impact

// Relevant parameters REQUIRED for the pipeline to run

params {
  input = '/path/to/samplesheet/'
  mode = '<wgs>|<targeted>'
  genomes = '<GRCh37_hmf>|<GRCh38_hmf>'
  outdir = '/path/to/outdir'
  fastp_umi = false // If pipeline starting from FASTQ files with UMIs, set to true
  redux_umi = false // If pipeline starting from BAM files with UMIs, set to true 
  monochromeLogs = false // To see colored logs, set to false
  max_fastq_records = 0 // When non-positive, will pass the fastq records as is to the alginer 
  // prepare_reference_only = false // If set to true, will only download and stage the resources for the run triggered and exit
}


// High-performance compute environment-specific configurations

// Singularity-speicifc arguments

singularity {
  enabled = true
  autoMounts = true
  runOptions = "-B </path/to/desired/mounted/volume/>"
  pullTimeout = '2h'
}

// Sets environment-specific variable to avoid timeouts during runs (resource staging or otherwise)
// Recommended

env {
  NXF_HTTP_TIMEOUT = '2h' // Increase HTTP download timeout
}


// Currently, all of the hmftools return a exit code of 1 on failure. 
// All other exit codes relate to Nextflow or the environment.

// Error handling & Strategy
// By default, upon failure, the pipeline will double the compute resource allocated to each process (regardless of label).
// The configurations below will avoid the pipeline failing immediately whenever encountering these common exit codes.

process {
  queue="compute"
  executor = "SLURM"
  errorStrategy = {task.exitStatus in [8,10,14,143,137,104,134,139,143,137,104,134,139,247,255] ? 'retry' : 'finish'}
  maxErrors = '-1'
  maxRetries = 3
 
  memory = { 8.GB * task.cpus }
  withName: '.*' {
    memory        = { 8.GB * task.cpus }
  }
}
params {
  max_cpus = 48
  max_memory = 384.GB
  max_time = 120.h
}


params {
    genomes {
        'GRCh37_hmf' {
            fasta         = "/path/to/Homo_sapiens.GRCh37.GATK.illumina.fasta"
            fai           = "/path/to/Homo_sapiens.GRCh37.GATK.illumina.fasta.fai"
            dict          = "/path/to/Homo_sapiens.GRCh37.GATK.illumina.fasta.dict"
            bwamem2_index = "/path/to/bwa-mem2_index/" // Required when aligning reads from FASTQ files (can be skipped when starting from BAM files)
            gridss_index  = "/path/to/gridss_index/" // Required if running VIRUSBREAKEND
            star_index    = "/path/to/star_index/" // Required only if RNA data included in analysis
        }
        'GRCh38_hmf' {
            fasta         = "/path/to/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"
            fai           = "/path/to/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai"
            dict          = "/path/to/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.dict"
            bwamem2_index = "/path/to/bwa-mem2_index/" // Required when aligning reads from FASTQ files (can be skipped when starting from BAM files)
            gridss_index  = "/path/to/gridss_index/" // Required if running VIRUSBREAKEND
            star_index    = "/path/to/star_index/" // Required only if RNA data included in analysis
        }
    }

    // Always required - will be staged from remote if not provided
    ref_data_virusbreakenddb_path = "/path/to/virusbreakend/" 
    ref_data_hmf_data_path = "/path/to/hmf_pipeline_resources/" 
}

// The pipeline will use the launch directory as the working directory by default. Can be overwritten as such.
workDir = '/path/to/work/'


// To overwrite compute resources for a single, several or all processes, it can be done as such:

// process selection
// 3 examples - 1) single process selection 2) ALL process selection 3) list of available processes 

// Selecting all processes
process{
    withName: '.*{
        memory = 16.GB
        cpus = 4
        time = 48.h
    }'
}

// process selection to overwrite label-specific compute resources specifications, e.g., overwrite the process_high etc 

process {
    withLabel: process_high {
        disk = 
        cpus = 
        memory = 
        time = 

    }

}


// GCP specific configuration that could fix 

// Error handling strategy
process {
    errorStrategy = { task.exitStatus in [8,10,14,143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries = 3
    maxErrors = '-1'
}

// Existing configurations for timeline, report, trace, and dag
timeline {
  enabled = true
  overwrite = true
  file = "${params.outdir}/pipeline_info/execution_timeline.html"
}

report {
  enabled = true
  overwrite = true
  file = "${params.outdir}/pipeline_info/execution_report.html"
}

trace {
  enabled = true
  overwrite = true
  file = "${params.outdir}/pipeline_info/execution_trace.txt"
}

dag {
  enabled = true
  overwrite = true
  file = "${params.outdir}/pipeline_info/pipeline_dag.svg"
}
